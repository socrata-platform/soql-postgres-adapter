# This should be basically the same for all secondaries

com.socrata.coordinator.secondary-watcher {
  instance = "{{ TRUTH_CLUSTER }}"

  // TODO: is would be nice if the secondary-watcher message-producer could
  // use this instead of its own zk configuration
  curator.ensemble = {{ ZOOKEEPER_ENSEMBLE }}
  service-advertisement.address = "{{ ARK_HOST }}"
  collocation.group = {{ TRUTH_CLUSTER_COLLOCATION_GROUP }}

  # So we have good news and bad news.
  # The bad news is in the docker world we don't have the notion of an instance that will come back and 
  # reclaim its work based on its own uuid when it restarts, ever new container will have a new uuid.
  # The good news is that once we accept that containers will never come back with their own UUID
  # and will be killed quickly, having a shorter claim timeout is ok despite still having a bug where
  # we stop the claim manager update when we start shutting down, not when we finish, so if we have
  # an extended shutdown our claim can be stolen.
  watcher-id = "{{ UUID }}"
  claim-timeout = "30m"

  database {
    # Running out of pooled connections can result in strange and difficult to detect failure conditions.
    # For example, the claim manager can get blocked resulting in claims not being updated and thus stolen.
    # We control concurrency by the number of workers combined with the number of secondary instances configured, so
    # we set it to a "essentially unlimited" value for the secondary watcher.
    c3p0.maxPoolSize = 10000
    host = "{{ DATA_COORDINATOR_DB_HOST }}"
    port = "{{ DATA_COORDINATOR_DB_PORT }}"
    database = "{{ DATA_COORDINATOR_DB_NAME }}"
    username = "{{ DATA_COORDINATOR_DB_USER }}"
    {{ DATA_COORDINATOR_DB_PASSWORD_LINE }}
    login-timeout = 10s
    connect-timeout = 10s
    cancel-signal-timeout = 10s
  }

  secondary {
    # unused
    defaultGroups = []
    groups = { }

    # The derived image will include the secondary configuration information necessary for the
    # specific secondary type.  We run a separate secondary watcher per secondary type for
    # isolation and to independently control scaling.
    include "{{ SERVER_ROOT }}/secondary.conf"
  }

  metrics {
    {% if SECONDARY_WATCHER_METRICS_PREFIX is defined -%}
    prefix = "{{ SECONDARY_WATCHER_METRICS_PREFIX }}"
    {%- endif %}
    enable-graphite = {{ ENABLE_GRAPHITE }}
    graphite-host = "{{ GRAPHITE_HOST }}"
    graphite-port = "{{ GRAPHITE_PORT }}"
    log-metrics = {{ LOG_METRICS }}
  }

  # When the secondary-watcher is configured with a message producer
  # it will send messages when secondary replication completes
  {% if AMQ_URL is defined and ZOOKEEPER_ENSEMBLE_STRING is defined -%}
  message-producer {
    eurybates {
      # has a default in Dockerfile
      producers = "{{ EURYBATES_PRODUCERS }}"
      activemq.connection-string = "{{ AMQ_URL }}"
    }
    zookeeper {
      # must be a string instead of an array
      # TODO: update secondary-watcher library so this can use ZOOKEEPER_ENSEMBLE used in secondary.conf
      conn-spec = "{{ ZOOKEEPER_ENSEMBLE_STRING }}"
      # has a default in Dockerfile
      session-timeout = "{{ ZOOKEEPER_SESSION_TIMEOUT }}"
    }
  }
  {%- endif %}
}
